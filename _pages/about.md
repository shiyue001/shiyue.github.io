---
permalink: /
title: "Welcome to Yue Shi's homepage"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
Biography
---------

I am a final-year direct Ph.D. student at Shanghai Jiao Tong University, advised by Prof. <a href="https://english.seiee.sjtu.edu.cn/english/detail/842_816.htm">Wenjun Zhang</a>, co-advised by Prof. <a href="https://scholar.google.com.sg/citations?user=eUbmKwYAAAAJ&hl=en">Bingbing Ni</a>. From November 2022 to June 2024, I was a visiting Ph.D. student at ETH Zurich, advised by Prof. <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en">Luc Van Gool</a> and Prof. <a href="https://scholar.google.com/citations?user=YYH0BjEAAAAJ&hl=en">Marc Pollefeys</a>. Before that, I received my bachelor's degree in information engineering from Xidian University, Xi'an, China, in 2019.

My research interests lie at the intersection of computer vision and computer graphics, including 3D reconstruction, novel view synthesis, 3D generation and editing.

<!-- News
----

<ul>
  <li>[04/2023] I have received 100 citations! </li>
  <li>[02/2023] One paper is accepted by CVPR2023. </li>
  <li>[10/2022] One paper is accepted by TIP. </li>
  <li>[06/2022] Two papers are accepted by ACM MM 2022.</li>
  <li>[12/2021] I am recognized as an outstanding student of Fudan University.</li>
  <li>[06/2021] One paper is accepted by ACM MM 2021.</li>
  <li>[04/2021] One paper is accepted by ICMR 2021.</li>
  <li>[07/2020] One paper is accepted by ACM MM 2020.</li>
  <li>[12/2019] I am awarded the Chinese National Scholarship.</li>
  <li>[07/2019] One paper is accepted by ACM MM 2019.</li>
</ul> -->

<!-- Selected Publications
# To update this
------
<div class="img">
            <div id="pic" class="baguetteBox gallery">
                <img src="images/framework-MED2N.png" />
            </div>   
            <div class="details" >
            <p><a href="">ME-D2N: Multi-Expert Domain Decompositional Network for Cross-Domain Few-Shot Learning</a><br /><strong>Yue Shi</strong>, Yu Xie, Yanwei Fu, Jingjing Chen, Yu-Gang Jiang<br /> ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2022.<br /> [<a href="">Paper Coming Soon</a>][<a href="https://github.com/lovelyqian/ME-D2N_for_CDFSL">Code</a>]</p>
            </div>

</div> -->

Research
------------

<table style="width:120%">
  <tr>
    <th width="40%">
      <img src="../images/图1.png" width="350"/>
    </th>
    <th style="text-align:left" width="60%">
            <span style="font-size:18px">Geometric granularity aware pixel-to-mesh</span><br>
            <span style="font-size:16px">Yue Shi<span style="font-weight:normal">, Bingbing Ni, Jinxian Liu, Dingyi Rong, Ye Qian, Wenjun Zhang</span></span><br>
             <span style="font-weight:normal;font-size:16px">Proceedings of the IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>), 2021.</span><br>
            <span style="font-weight:normal;font-size:16px">[<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Shi_Geometric_Granularity_Aware_Pixel-To-Mesh_ICCV_2021_paper.pdf">Paper</a>]</span>
    </th>
  </tr> 
</table>

<table style="width:120%">
  <tr>
    <th width="40%">
      <img src="../images/3dv.png" width="350"/>
    </th>
    <th style="text-align:left" width="60%">
            <span style="font-size:18px">Mipmap-GS: Let Gaussians Deform with Scale-specific Mipmap for Anti-aliasing Rendering</span><br>
            <span style="font-size:16px">
  <span style="font-weight:normal">Jiameng Li<span style="position:relative; top:-2px; left:5px;">*</span>, 
  <strong><span style="position:relative; top:0; left:5px;">Yue Shi</span><span style="position:relative; top:-2px; left:5px;">*</span></strong>, 
  Jiezhang Cao, Bingbing Ni, Wenjun Zhang, Kai Zhang, Luc Van Gool</span>
</span><br>
             <span style="font-weight:normal;font-size:16px">International Conference on 3D Vision(<strong>3DV</strong>), 2021.</span><br>
            <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/abs/2408.06286">Paper</a>]</span>
    </th>
  </tr> 
</table>


<table style="width:120%">
  <tr>
    <th width="40%">
      <img src="../images/garf.png" width="600"/>
    </th>
    <th style="text-align:left" width="60%">
            <span style="font-size:18px">GARF: Geometry-aware generalized neural radiance field</span><br>
            <span style="font-size:16px">Yue Shi<span style="font-weight:normal">, Dingyi Rong, Bingbing Ni, Chang Chen, Wenjun Zhang</span></span><br>
            <span style="font-weight:normal;font-size:16px">Displays, Under Review (Minor Revision).</span><br>
            <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/pdf/2212.02280.pdf">Paper</a>]</span>
    </th>
  </tr> 
</table>

<table style="width:120%">
  <tr>
    <th width="40%">
      <img src="../images/edit.png" width="350"/>
    </th>
    <th style="text-align:left" width="60%">
            <span style="font-size:18px">Learning Geometry and Appearance for Improved Radiance Fields Editing</span><br>
            <span style="font-size:16px">Yue Shi<span style="font-weight:normal">, Rui Shi, Yuxuan Xiong, Jiezhang Cao, Bingbing Ni, Wenjun Zhang, Luc Van Gool</span></span><br>
      <span style="font-weight:normal;font-size:16px">International Journal of Computer Vision (IJCV), Under Review.</span><br>      
      <span style="font-weight:normal;font-size:16px">[<a href="https://drive.google.com/file/d/1hVSAEM82ibnsklURHG0SC8ZoE7RYStew/view?usp=drive_link">Demo</a>]</span>
    </th>
  </tr> 
</table>

<table style="width:120%">
  <tr>
    <th width="40%">
      <img src="../images/图3.png" width="350"/>
    </th>
    <th style="text-align:left" width="60%">
            <span style="font-size:18px">USR: Unsupervised separated 3d garment and human reconstruction via geometry and semantic consistency</span><br>
            <span style="font-size:16px">Yue Shi<span style="font-weight:normal">, Yuxuan Xiong, Bingbing Ni, Wenjun Zhang</span></span><br>
            <span style="font-weight:normal;font-size:16px">arXiv preprint, 2023.</span><br>
                  <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/pdf/2302.10518.pdf">Paper</a>]</span>
    </th>
  </tr> 
</table>



<table style="width:120%">
  <tr>
    <th width="40%">
      <img src="../images/focal.png" width="350"/>
    </th>
    <th style="text-align:left" width="60%">
            <span style="font-size:18px">FocalDreamer: Text-driven 3D Editing via Focal-fusion Assembly</span><br>
            <span style="font-size:16px"><span style="font-weight:normal">Yuhan Li，Yishun Dou</span>, Yue Shi<span style="font-weight:normal">, Yu Lei, Xuanhong Chen, Yi Zhang, Peng Zhou, Bingbing Ni</span></span><br>
            <span style="font-weight:normal;font-size:16px">Proceedings of the AAAI Conference on Artificial Intelligence(AAAI), 2024.</span><br>
            <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/pdf/2308.10608.pdf">Paper</a>]</span>
    </th>
  </tr> 
</table>

<table style="width:120%">
  <tr>
    <th width="40%">
      <img src="../images/sr.png" width="350"/>
    </th>
    <th style="text-align:left" width="60%">
            <span style="font-size:18px">Deep Equilibrium Diffusion Restoration with Parallel Sampling</span><br>
            <span style="font-size:16px"><span style="font-weight:normal">Jiezhang Cao</span>, Yue Shi<span style="font-weight:normal">, Kai Zhang, Yulun Zhang, Radu Timofte, Luc Van Gool</span></span><br>
            <span style="font-weight:normal;font-size:16px">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.</span><br>
            <span style="font-weight:normal;font-size:16px">[<a href="[https://arxiv.org/pdf/2308.10608.pdf](https://openaccess.thecvf.com/content/CVPR2024/html/Cao_Deep_Equilibrium_Diffusion_Restoration_with_Parallel_Sampling_CVPR_2024_paper.html)">Paper</a>]</span>
    </th>
  </tr> 
</table>

<table style="width:120%">
  <tr>
    <th width="40%">
      <img src="../images/sticker.png" width="350"/>
    </th>
    <th style="text-align:left" width="60%">
            <span style="font-size:18px">InstantSticker: Realistic Decal Blending via Disentangled Object Reconstruction</span><br>
            <span style="font-size:16px"><span style="font-weight:normal">Yi Zhang, Xiaoyang Huang, Yishun Dou</span>, Yue Shi<span style="font-weight:normal">, Rui Shi, Ye Chen, Bingbing Ni, Wenjun Zhang</span></span><br>
            <span style="font-weight:normal;font-size:16px">Proceedings of the AAAI Conference on Artificial Intelligence(AAAI), 2025.</span><br>
    </th>
  </tr> 
</table>









<!-- <ul>
  <li>
    <p><a href="">ME-D2N: Multi-Expert Domain Decompositional Network for Cross-Domain Few-Shot Learning</a><br /><strong>Yue Shi</strong>, Yu Xie, Yanwei Fu, Jingjing Chen, Yu-Gang Jiang<br /> ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2022.<br /> [<a href="">Paper Coming Soon</a>][<a href="https://github.com/lovelyqian/ME-D2N_for_CDFSL">Code</a>]</p>
  </li>
  <li>
    <p><a href="">TGDM: Target Guided Dynamic Mixup for Cross-Domain Few-Shot Learning</a><br />Linhai Zhuo, <strong>Yue Shi</strong>, Jingjing Chen, Yixin Cao, Yu-Gang Jiang<br /> ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2022.<br /> [<a href="">Paper Coming Soon</a>]</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2203.07656">Wave-SAN: Wavelet based Style Augmentation Network for Cross-Domain Few-Shot Learning</a><br /> <strong>Yue Shi</strong>, Yu Xie, Yanwei Fu, Jingjing Chen, Yu-Gang Jiang<br /> arXiv preprint, 2022. <br /> [<a href="https://arxiv.org/pdf/2203.07656.pdf">Paper</a>]</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.11978">Meta-FDMixup: Cross-Domain Few-Shot Learning Guided by Labeled Target Data</a><br /><strong>Yue Shi</strong>, Yanwei Fu, Yu-Gang Jiang<br /> ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2021. <br /> [<a href="https://arxiv.org/pdf/2107.11978.pdf">Paper</a>][<a href="https://github.com/lovelyqian/Meta-FDMixup">Code</a>][<a href="https://www.youtube.com/watch?v=G8Mlde4FpsU">Youtube Video</a>][<a href="https://www.bilibili.com/video/BV1xT4y1f7B6?spm_id_from=333.999.0.0&vd_source=668a0bb77d7d7b855bde68ecea1232e7">Bilibili Video</a>]</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.11756">Can Action be Imitated? Learn to Reconstruct and Transfer Human Dynamics from Videos</a><br /><strong>Yue Shi</strong>, Yanwei Fu, Yu-Gang Jiang<br /> International Conference on Multimedia Retrieval (<strong>ICMR</strong>). 2021. (<strong>Oral</strong>)<br /> [<a href="https://arxiv.org/pdf/2107.11756.pdf">Paper</a>][<a href="https://www.bilibili.com/video/BV1VY41147xt?spm_id_from=333.999.0.0">Bilibili Video</a>]</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2010.09982">Depth Guided Adaptive Meta-Fusion Network for Few-shot Video Recognition
</a><br /><strong>Yue Shi</strong>, Li Zhang, Junke Wang, Yanwei Fu, Yu-Gang Jiang<br /> ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2020. (<strong>Oral</strong>)<br /> [<a href="https://arxiv.org/pdf/2010.09982.pdf">Paper</a>][<a href="https://github.com/lovelyqian/AMeFu-Net">Code</a>][<a href="https://www.youtube.com/watch?v=KqNYuZD5xdw">Youtube Video</a>][<a href="https://www.bilibili.com/video/BV1i44y1t78U?spm_id_from=333.999.0.0">Bilibili Video</a>]</p>
  </li>
  <li>
    <p><a href="http://www.cs.cmu.edu/~yuxiongw/research/Embodied_One-Shot_Video_Recognition_Learning_from_Actions_of_a_Virtual_Embodied_Agent.pdf"> Embodied One-Shot Video Recognition: Learning from Actions of a Virtual Embodied Agent </a><br /> <strong>Yue Shi</strong>, Chengrong Wang, Yanwei Fu, Yu-Xiong Wang, Cong Bai, Xiangyang Xue, Yu-Gang Jiang<br /> ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2019. (<strong>Oral</strong>) <br /> [<a href="http://www.cs.cmu.edu/~yuxiongw/research/Embodied_One-Shot_Video_Recognition_Learning_from_Actions_of_a_Virtual_Embodied_Agent.pdf">Paper</a>][<a href="https://github.com/lovelyqian/Embodied-One-Shot-Video-Recognition">Code</a>][<a href="http://www.sdspeople.fudan.edu.cn/fuyanwei/dataset/UnrealAction/">UnrealAction Dataset</a>]</p>
  </li>
</ul> -->
